{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/google/gemma-7b/blob/main/examples/notebook_sft_peft.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c54030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "os.environ[\"HF_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c541e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q -U bitsandbytes==0.43.3\n",
    "!pip3 install -q -U peft==0.12.0\n",
    "!pip3 install -q -U trl==0.9.6\n",
    "!pip3 install -q -U accelerate==0.33.0\n",
    "!pip3 install -q -U datasets==2.21.0\n",
    "!pip3 install -q -U transformers==4.44.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936cd19-eb58-4569-b83c-24450709aa3f",
   "metadata": {},
   "source": [
    "### Create & verify user prompt from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1d2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(inputs: dict) -> str:\n",
    "    \"\"\"\n",
    "    Function that creates prompt for poetry explanation.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    You are given the poem \"{title}\" by \"{poet}\".\n",
    "    <poem>\n",
    "    {content_before}\n",
    "    {referent}\n",
    "    {context_after}\n",
    "    </poem>\n",
    "    Explain the meaning of the following lines: \"{referent}\"\n",
    "    \"\"\".format(\n",
    "        title=inputs['poem_title'],\n",
    "        poet=inputs['poet'],\n",
    "        content_before=inputs['content_before'],\n",
    "        context_after=inputs['context_after'],\n",
    "        referent=inputs['referent']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ade6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    'content_before': \"The battle rent a cobweb diamond-strung\\nAnd cut a flower beside a ground bird's nest\\nBefore it stained a single human breast.\\nThe stricken flower bent double and so hung.\\nAnd still the bird revisited her young.\\nA butterfly its fall had dispossessed\\nA moment sought in air his flower of rest,\\nThen lightly stooped to it and fluttering clung.\\nOn the bare upland pasture there had spread\\nO'ernight 'twixt mullein stalks a wheel of thread\\nAnd straining cables wet with silver dew.\",\n",
    "    'referent': 'A sudden passing bullet shook it dry.',\n",
    "    'context_after': 'The indwelling spider ran to greet the fly,\\nBut finding nothing, sullenly withdrew.',\n",
    "    'annotation': 'The serenity is, as the reader no doubt anticipates, broken by the shot described in this snappy line. The dryness may represent the loss of a source of life that invigorates the natural — and human — worlds.',\n",
    "    'poet': 'Robert Frost',\n",
    "    'poem_title': 'Range-finding'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d79b32c-79e9-42e1-9dbb-b5be9da635d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are given the poem \"Range-finding\" by \"Robert Frost\".\\n    <poem>\\n    The battle rent a cobweb diamond-strung\\nAnd cut a flower beside a ground bird\\'s nest\\nBefore it stained a single human breast.\\nThe stricken flower bent double and so hung.\\nAnd still the bird revisited her young.\\nA butterfly its fall had dispossessed\\nA moment sought in air his flower of rest,\\nThen lightly stooped to it and fluttering clung.\\nOn the bare upland pasture there had spread\\nO\\'ernight \\'twixt mullein stalks a wheel of thread\\nAnd straining cables wet with silver dew.\\n    A sudden passing bullet shook it dry.\\n    The indwelling spider ran to greet the fly,\\nBut finding nothing, sullenly withdrew.\\n    </poem>\\n    Explain the meaning of the following lines: \"A sudden passing bullet shook it dry.\"\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c92d76",
   "metadata": {},
   "source": [
    "### Split dataset into train/validation/test without intersections between poets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4191f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_author(df, split_ratio=[0.7, 0.1]) -> list[pd.DataFrame]:\n",
    "    unique_poets_count = dict(df['poet'].value_counts())\n",
    "    \"\"\"Function that splits dataset into train/validation/test with no intersection between authors\"\"\"\n",
    "    \n",
    "    # set target counts for each subset\n",
    "    total_count = len(df)\n",
    "    count_deviation = total_count*0.01\n",
    "    train_count_target = int(total_count * split_ratio[0])\n",
    "    validation_count_target = int(total_count * split_ratio[1])\n",
    "    test_count_target = total_count - train_count_target - validation_count_target\n",
    "    train_poets, train_count = [], 0\n",
    "    validation_poets, validation_count = [], 0\n",
    "    \n",
    "    while abs(train_count-train_count_target) > count_deviation:\n",
    "        print('Selecting train dataset')\n",
    "        # define start values\n",
    "        train_poets, train_count = [], 0\n",
    "        unique_poets_list = df['poet'].value_counts().index.values.copy()\n",
    "\n",
    "        while train_count < train_count_target:\n",
    "            random_index = random.randint(0, len(unique_poets_list)-1)\n",
    "            train_poets.append(unique_poets_list[random_index])\n",
    "            train_count += unique_poets_count[unique_poets_list[random_index]]\n",
    "            unique_poets_list = np.delete(unique_poets_list, random_index)\n",
    "           \n",
    "    \n",
    "    while abs(validation_count-validation_count_target) > count_deviation:\n",
    "        print('Selecting validation dataset')\n",
    "        validation_poets, validation_count = [], 0\n",
    "        val_unique_poets_list = unique_poets_list.copy()\n",
    "        \n",
    "        while validation_count < validation_count_target:\n",
    "            random_index = random.randint(0, len(val_unique_poets_list)-1)\n",
    "            validation_poets.append(val_unique_poets_list[random_index])\n",
    "            validation_count += unique_poets_count[val_unique_poets_list[random_index]]\n",
    "            val_unique_poets_list = np.delete(val_unique_poets_list, random_index)\n",
    "    \n",
    "    # all left poets are for testing\n",
    "    test_poets = val_unique_poets_list\n",
    "    \n",
    "    print(train_count, len(df[df['poet'].isin(train_poets)]))\n",
    "    print(set(train_poets).intersection(validation_poets))\n",
    "    print(f\"Allowed deviation = {count_deviation}\")\n",
    "    print(f\"Train count (target={train_count_target}) = {len(df[df['poet'].isin(train_poets)])}\")\n",
    "    print(f\"Validation count (target={validation_count_target}) = {len(df[df['poet'].isin(validation_poets)])}\")\n",
    "    print(f\"Test count (target={test_count_target}) = {len(df[df['poet'].isin(test_poets)])}\")\n",
    "        \n",
    "    return df[df['poet'].isin(train_poets)], df[df['poet'].isin(validation_poets)], df[df['poet'].isin(test_poets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "62071ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting train dataset\n",
      "Selecting validation dataset\n",
      "Selecting validation dataset\n",
      "2576 2576\n",
      "set()\n",
      "Allowed deviation = 36.29\n",
      "Train count (target=2540) = 2576\n",
      "Validation count (target=362) = 366\n",
      "Test count (target=727) = 687\n"
     ]
    }
   ],
   "source": [
    "train_df, validation_df, test_df = split_by_author(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "68a3a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/annotations_dataset_train.csv', index=False)\n",
    "validation_df.to_csv('./data/annotations_dataset_validation.csv', index=False)\n",
    "test_df.to_csv('./data/annotations_dataset_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8922b9c",
   "metadata": {},
   "source": [
    "### Create HF dataset from train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4d954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": [\"./data/annotations_dataset_train.csv\"],\n",
    "             \"test\": [\"./data/annotations_dataset_test.csv\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2346a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-dev\\miniconda3\\envs\\llm_sft\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 2576 examples [00:00, 65997.56 examples/s]\n",
      "Generating test split: 687 examples [00:00, 21817.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd567da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2576, 687)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e96dc",
   "metadata": {},
   "source": [
    "### Load the  base model from HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3dc109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
    "\n",
    "model_id = 'google/gemma-2-2b-it'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'], add_eos_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             #quantization_config=bnb_config,\n",
    "                                             device_map='cuda',\n",
    "                                             token=os.environ['HF_TOKEN'],\n",
    "                                             attn_implementation='eager',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e35fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n    You are given the poem \"Lenox Avenue: Midnight\" by \"Langston Hughes\".\\n    <poem>\\n    \\n    The rhythm of life\\nIs a jazz rhythm,\\n    Honey.\\nThe gods are laughing at us.\\nThe broken heart of love,\\n    </poem>\\n    Explain the meaning of the following lines: \"The rhythm of life\\nIs a jazz rhythm,\"\\n    '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f8926a1-db83-47f5-97ea-46de55e2c991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4cf8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are given the poem \"Lenox Avenue: Midnight\" by \"Langston Hughes\".\n",
      "    <poem>\n",
      "    \n",
      "    The rhythm of life\n",
      "Is a jazz rhythm,\n",
      "    Honey.\n",
      "The gods are laughing at us.\n",
      "The broken heart of love,\n",
      "    </poem>\n",
      "    Explain the meaning of the following lines: \"The rhythm of life\n",
      "Is a jazz rhythm,\"\n",
      "* **What is the meaning of the line?**\n",
      "* **How does this line relate to the overall theme of the poem?**\n",
      "\n",
      "Here's a breakdown of the poem:\n",
      "\n",
      "* **\"The rhythm of life is a jazz rhythm, honey.\"** This line sets the tone for the poem, suggesting a sense of improvisation, spontaneity, and a certain unpredictability in life.\n",
      "* **\"The gods are laughing at us.\"** This line suggests a sense of irony and perhaps even despair.\n",
      "* **\"The broken heart of love.\"** This line suggests a sense of loss and pain.\n",
      "\n",
      "\n",
      "Let me know if you'd like to explore any other lines from the poem! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text = create_prompt(inputs=dict(df.iloc[1500]))\n",
    "device = \"cuda\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n",
    "#inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898541d-610a-4a63-8bae-cf0ae8888801",
   "metadata": {},
   "source": [
    "### Apply & verify chat template to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95184a12-e706-43df-b721-518f1d685ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_before': Value(dtype='string', id=None),\n",
       " 'referent': Value(dtype='string', id=None),\n",
       " 'context_after': Value(dtype='string', id=None),\n",
       " 'annotation': Value(dtype='string', id=None),\n",
       " 'poet': Value(dtype='string', id=None),\n",
       " 'poem_title': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db98c9a1-a409-4d16-9aa7-c8cd1ac4e546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying chat template: 100%|█████████████████████████████████████████████| 2576/2576 [00:00<00:00, 7536.67 examples/s]\n",
      "Applying chat template: 100%|███████████████████████████████████████████████| 687/687 [00:00<00:00, 8402.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def apply_chat_template(example, tokenizer):\n",
    "    text = create_prompt(example)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "        {\"role\": \"assistant\", \"content\": example['annotation']}\n",
    "    ]\n",
    "    \n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return example\n",
    "\n",
    "column_names = list(dataset[\"train\"].features)\n",
    "dataset = dataset.map(apply_chat_template,\n",
    "                      fn_kwargs={\"tokenizer\": tokenizer},\n",
    "                      remove_columns=column_names,\n",
    "                      desc=\"Applying chat template\"\n",
    "                     )\n",
    "\n",
    "# create the splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785628d1-caa1-489e-b800-0838195c8f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 230 of the processed training set:\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "You are given the poem \"Crimson\" by \"Carl Sandburg\".\n",
      "    <poem>\n",
      "    None\n",
      "    Crimson is the slow smolder of the cigar end I hold,\n",
      "    Gray is the ash that stiffens and covers all silent the fire.\n",
      "(A great man I know is dead and while he lies in his coffin a gone flame I sit here in cumbering shadows and smoke and watch my thoughts come and go.)\n",
      "    </poem>\n",
      "    Explain the meaning of the following lines: \"Crimson is the slow smolder of the cigar end I hold,\"<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Crimson, a deep red (like blood) and the  slow  burning – life is  long , not short; though inevitably it does come to an end \n",
      " Cigars  are the  perfect  tobacco product for this poem: cigarettes burn too quickly,  Black & Milds  bring along the notion of poverty, and  blunts are for rappers<end_of_turn>\n",
      "\n",
      "Sample 429 of the processed training set:\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "You are given the poem \"Brass Spittoons\" by \"Langston Hughes\".\n",
      "    <poem>\n",
      "    Clean the spittoons, boy.\n",
      "Detroit,\n",
      "Chicago,\n",
      "Atlantic City,\n",
      "Palm Beach.\n",
      "Clean the spittoons.\n",
      "The steam in hotel kitchens,\n",
      "And the smoke in hotel lobbies,\n",
      "And the slime in hotel spittoons:\n",
      "Part of my life.\n",
      "Hey, boy!\n",
      "    A nickel,\n",
      "A dime,\n",
      "A dollar,\n",
      "Two dollars a day\n",
      "    Buy shoes for the baby.\n",
      "House rent to pay.\n",
      "Gin on Saturday,\n",
      "    </poem>\n",
      "    Explain the meaning of the following lines: \"A nickel,\n",
      "A dime,\n",
      "A dollar,\n",
      "Two dollars a day\"<end_of_turn>\n",
      "<start_of_turn>model\n",
      "This is possibly a reference to tips patrons give the boy, or it could indicate he is finding money as he cleans the spittoons. \n",
      " See this  newspaper article on the poet for more.<end_of_turn>\n",
      "\n",
      "Sample 1597 of the processed training set:\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "You are given the poem \"The Grass\" by \"Emily Dickinson\".\n",
      "    <poem>\n",
      "    The Grass so little has to do\n",
      "A Sphere of simple Green\n",
      "With only Butterflies to brood\n",
      "And Bees to entertain\n",
      "And stir all day to pretty Tunes\n",
      "The Breezes fetch along\n",
      "And hold the Sunshine in its lap\n",
      "And bow to everything\n",
      "And thread the Dews, all night, like Pearls\n",
      "And make itself so fine\n",
      "A Duchess were too common\n",
      "For such a noticing\n",
      "    And even when it dies – to pass\n",
      "In Odors so divine\n",
      "    Like Lowly spices, lain to sleep\n",
      "Or Spikenards, perishing\n",
      "And then, in Sovereign Barns to dwell\n",
      "    </poem>\n",
      "    Explain the meaning of the following lines: \"And even when it dies – to pass\n",
      "In Odors so divine\"<end_of_turn>\n",
      "<start_of_turn>model\n",
      "This mid-line break is interesting because it forces us to think about the implications of ‘dies’. This metaphorical woman is so lovely that even when she dies she smells amazing–another way to reinforce her wealth–and she’s so important that her death causes the death of the plants used to perfume her body.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in random.sample(range(len(dataset[\"train\"])), 3):\n",
    "    print(f\"Sample {index} of the processed training set:\\n\\n{dataset['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c406275-b508-4e6c-9100-db332989b01b",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d793d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59e7b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 2576/2576 [00:00<00:00, 9347.44 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 687/687 [00:00<00:00, 9932.57 examples/s]\n",
      "C:\\Users\\vlad-dev\\miniconda3\\envs\\llm_sft\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vlad-dev\\miniconda3\\envs\\llm_sft\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 2,576\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 64\n",
      "  Total optimization steps = 40\n",
      "  Number of trainable parameters = 41,533,440\n",
      "C:\\Users\\vlad-dev\\miniconda3\\envs\\llm_sft\\Lib\\site-packages\\torch\\utils\\checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 21:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.441600</td>\n",
       "      <td>2.415272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs\\checkpoint-40\n",
      "loading configuration file config.json from cache at C:\\Users\\vlad-dev\\.cache\\huggingface\\hub\\models--google--gemma-2-2b-it\\snapshots\\e48216d9004e7fd70bc4fdfdc5b7cc3349f8e619\\config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in outputs\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in outputs\\checkpoint-40\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 687\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer, SFTConfig \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "torch.cuda.empty_cache()\n",
    "sft_config = SFTConfig(\n",
    "    max_seq_length=512,\n",
    "    warmup_steps=2,\n",
    "    output_dir=\"outputs\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=64,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    dataset_text_field=\"text\",\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    seed=42,\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config\n",
    ")\n",
    "\n",
    "\n",
    "#model.config.use_cache = False\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0182e4e4-cfa3-457e-9d4a-95f790778ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     0.9938\n",
      "  total_flos               =  7334179GF\n",
      "  train_loss               =      2.689\n",
      "  train_runtime            = 0:22:16.15\n",
      "  train_samples            =       2576\n",
      "  train_samples_per_second =      1.928\n",
      "  train_steps_per_second   =       0.03\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0e3e1-d991-446c-b113-9e596f91e9ed",
   "metadata": {},
   "source": [
    "### Run inference on fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6822990-bc4b-43d7-91aa-6ed2a47a5ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\vlad-dev\\.cache\\huggingface\\hub\\models--google--gemma-2-2b-it\\snapshots\\e48216d9004e7fd70bc4fdfdc5b7cc3349f8e619\\config.json\n",
      "Model config Gemma2Config {\n",
      "  \"_name_or_path\": \"google/gemma-2-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\vlad-dev\\.cache\\huggingface\\hub\\models--google--gemma-2-2b-it\\snapshots\\e48216d9004e7fd70bc4fdfdc5b7cc3349f8e619\\model.safetensors.index.json\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"pad_token_id\": 0,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "All model checkpoint weights were used when initializing Gemma2ForCausalLM.\n",
      "\n",
      "All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at google/gemma-2-2b-it.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\vlad-dev\\.cache\\huggingface\\hub\\models--google--gemma-2-2b-it\\snapshots\\e48216d9004e7fd70bc4fdfdc5b7cc3349f8e619\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"./outputs/checkpoint-40\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, add_eos_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir,\n",
    "                                             device_map=\"cuda\",\n",
    "                                             attn_implementation='eager',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2f95da3-94af-4520-95f5-4f5f0ca0bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('./data/annotations_dataset_validation.csv').fillna('').sample(n=1).to_dict(orient='records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b61707ca-9fe4-4441-bb0c-66a02a5bcb42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are given the poem \"On Death\" by \"John Keats\".\n",
      "    <poem>\n",
      "    Can death be sleep, when life is but a dream,\n",
      "    And scenes of bliss pass as a phantom by?\n",
      "    The transient pleasures as a vision seem,\n",
      "And yet we think the greatest pain's to die.\n",
      "How strange it is that man on earth should roam,\n",
      "    </poem>\n",
      "    Explain the meaning of the following lines: \"And scenes of bliss pass as a phantom by?\"\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(create_prompt(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3404c0d8-5d84-4683-9353-7b546d8fd355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are given the poem \"On Death\" by \"John Keats\".\n",
      "    <poem>\n",
      "    Can death be sleep, when life is but a dream,\n",
      "    And scenes of bliss pass as a phantom by?\n",
      "    The transient pleasures as a vision seem,\n",
      "And yet we think the greatest pain's to die.\n",
      "How strange it is that man on earth should roam,\n",
      "    </poem>\n",
      "    Explain the meaning of the following lines: \"And scenes of bliss pass as a phantom by?\"\n",
      "model\n",
      "The speaker is comparing death to a dream, and death to a phantom.  The phantom is a ghost, and a ghost is a spirit that is not alive.  The phantom is also a thing that is not real, and a thing that is not real is not alive.  The speaker is saying that death is not real, and that death is not alive.  The speaker is also saying that death is not a thing that is real, and that death is not a thing that is alive.  The speaker is also saying that death is not a thing that is real, and that death is not a thing that is alive.  The speaker is also saying that death is not a thing that is real, and that death is not a thing that is alive.  The speaker is also saying that death is not a thing that is real, and that death is not a thing that is alive.  The speaker is also saying that death is not a thing that is real, and that death is not a thing that is alive.  The speaker is also saying that death is not a thing that is real, and that death is not a thing that is alive.  The speaker is also saying that death is not a thing that is real, and\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "text = create_prompt(sample)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n",
    "#inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=256,\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289d664-9656-483a-8397-abfc70740858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
